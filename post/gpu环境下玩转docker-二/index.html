<!DOCTYPE html>
<html lang="en" itemscope itemtype="http://schema.org/WebPage">
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>GPU环境下玩转Docker(二) - CloudNativeOps - SRE，DevOps、Kubernetes 云原生运维。成长之路，一起前行！</title>
  

<meta name="renderer" content="webkit" />
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1"/>

<meta name="MobileOptimized" content="width"/>
<meta name="HandheldFriendly" content="true"/>


<meta name="applicable-device" content="pc,mobile">

<meta name="theme-color" content="#f8f5ec" />
<meta name="msapplication-navbutton-color" content="#f8f5ec">
<meta name="apple-mobile-web-app-capable" content="yes">
<meta name="apple-mobile-web-app-status-bar-style" content="#f8f5ec">

<meta name="mobile-web-app-capable" content="yes">

<meta name="author" content="BGBiao" />
  <meta name="description" content=" 前言: 在一节中,我们已经在GPU物理机上准备好了GPU环境，本篇文章介绍如何使用Docker来管理GPU容器。
" />

  <meta name="keywords" content="SRE, Ops, DevOps, Kubernetes, Docker, CloudNative" />






<meta name="generator" content="Hugo 0.103.1" />


<link rel="canonical" href="https://bgbiao.top/post/gpu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%8E%A9%E8%BD%ACdocker-%E4%BA%8C/" />





<link rel="icon" href="/favicon.ico" />











<link rel="stylesheet" href="/sass/jane.min.444f741ba7b684e63dff9a3b61ec1fb750c9b563b5a0b64153f1af8025c5fac1.css" integrity="sha256-RE90G6e2hOY9/5o7Yewft1DJtWO1oLZBU/GvgCXF&#43;sE=" media="screen" crossorigin="anonymous">




<link rel="stylesheet" href="/css/custom.css">


<meta property="og:title" content="GPU环境下玩转Docker(二)" />
<meta property="og:description" content="
前言:
在一节中,我们已经在GPU物理机上准备好了GPU环境，本篇文章介绍如何使用Docker来管理GPU容器。
" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://bgbiao.top/post/gpu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%8E%A9%E8%BD%ACdocker-%E4%BA%8C/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2017-10-26T10:07:46+00:00" />
<meta property="article:modified_time" content="2020-11-01T17:14:34+08:00" />

<meta itemprop="name" content="GPU环境下玩转Docker(二)">
<meta itemprop="description" content="
前言:
在一节中,我们已经在GPU物理机上准备好了GPU环境，本篇文章介绍如何使用Docker来管理GPU容器。
"><meta itemprop="datePublished" content="2017-10-26T10:07:46+00:00" />
<meta itemprop="dateModified" content="2020-11-01T17:14:34+08:00" />
<meta itemprop="wordCount" content="3857">
<meta itemprop="keywords" content="Docker,GPU,NVIDIA,NVIDIA-Docker," /><meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="GPU环境下玩转Docker(二)"/>
<meta name="twitter:description" content="
前言:
在一节中,我们已经在GPU物理机上准备好了GPU环境，本篇文章介绍如何使用Docker来管理GPU容器。
"/>

<!--[if lte IE 9]>
  <script src="https://cdnjs.cloudflare.com/ajax/libs/classlist/1.1.20170427/classList.min.js"></script>
<![endif]-->

<!--[if lt IE 9]>
  <script src="https://cdn.jsdelivr.net/npm/html5shiv@3.7.3/dist/html5shiv.min.js"></script>
  <script src="https://cdn.jsdelivr.net/npm/respond.js@1.4.2/dest/respond.min.js"></script>
<![endif]-->


<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	window.ga=window.ga||function(){(ga.q=ga.q||[]).push(arguments)};ga.l=+new Date;
	ga('create', 'UA-151318936-1', 'auto');
	
	ga('send', 'pageview');
}
</script>
<script async src='https://www.google-analytics.com/analytics.js'></script>



</head>
<body>
  <div id="mobile-navbar" class="mobile-navbar">
  <div class="mobile-header-logo">
    <a href="/" class="logo">BGBiao的SRE人生</a>
  </div>
  <div class="mobile-navbar-icon">
    <span></span>
    <span></span>
    <span></span>
  </div>
</div>
<nav id="mobile-menu" class="mobile-menu slideout-menu">
  <ul class="mobile-menu-list">
    <li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/">首页</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/post/">全部文章</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/categories/">分类</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          <div class="mobile-menu-parent">
            <span class="mobile-submenu-open"></span>
            <a href="https://bgbiao.top/">
              学习文档
            </a>
          </div>
          <ul class="mobile-submenu-list">
            
              <li>
                <a href="https://otexts.com/fppcn/index.html">&lt;预测:方法与实践&gt;</a>
              </li>
            
          </ul>
        
      </li><li class="mobile-menu-item">
        
          
          <div class="mobile-menu-parent">
            <span class="mobile-submenu-open"></span>
            <a href="https://bgbiao.top/">
              Golang相关
            </a>
          </div>
          <ul class="mobile-submenu-list">
            
              <li>
                <a href="https://gowebexamples.com/">Golang web开发示例</a>
              </li>
            
              <li>
                <a href="http://go-database-sql.org/">Golang SQL开发教程</a>
              </li>
            
              <li>
                <a href="https://rpcx.io/">Golang RPCX框架</a>
              </li>
            
              <li>
                <a href="https://github.com/talkgo/read">Golang 夜读</a>
              </li>
            
              <li>
                <a href="https://books.studygolang.com/">Golang 开源书籍</a>
              </li>
            
          </ul>
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/devops/">DevOps/SRE</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/tags/">标签</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/books/">我的书单</a>
          
        
      </li><li class="mobile-menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/about/">关于我</a>
          
        
      </li>
    

    
      <li class="mobile-menu-item">
        <a id="openSearchMobile" class="mobile-menu-item-link menu-item-search" href="#">
          <i class="iconfont">
            <svg version="1.1" viewBox="0 0 1024 1024"
  xmlns="http://www.w3.org/2000/svg" width="18" height="18"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M973.81454219 973.81454219a91.78207815 91.78207815 0 0 1-129.80999631 0l-161.97482118-161.97482118a425.48527711 425.48527711 0 0 1-230.35931791 68.16531768 428.3346319 428.3346319 0 1 1 428.3346319-428.3346319 425.48527711 425.48527711 0 0 1-68.16531768 230.35931791l162.02961656 161.97482118a91.83687354 91.83687354 0 0 1-0.05479538 129.80999631zM451.67040679 145.69361559a305.97679241 305.97679241 0 1 0 0 611.95358361 305.97679241 305.97679241 0 0 0 0-611.95358361z">
  </path>
</svg>

          </i>
        </a>
      </li>
    
  </ul>
</nav>


  
    






  <link rel="stylesheet" href="/lib/photoswipe/photoswipe.min.css" />
  <link rel="stylesheet" href="/lib/photoswipe/default-skin/default-skin.min.css" />




<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

<div class="pswp__bg"></div>

<div class="pswp__scroll-wrap">
    
    <div class="pswp__container">
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
      <div class="pswp__item"></div>
    </div>
    
    <div class="pswp__ui pswp__ui--hidden">
    <div class="pswp__top-bar">
      
      <div class="pswp__counter"></div>
      <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>
      <button class="pswp__button pswp__button--share" title="Share"></button>
      <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>
      <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>
      
      
      <div class="pswp__preloader">
        <div class="pswp__preloader__icn">
          <div class="pswp__preloader__cut">
            <div class="pswp__preloader__donut"></div>
          </div>
        </div>
      </div>
    </div>
    <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
      <div class="pswp__share-tooltip"></div>
    </div>
    <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
    </button>
    <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
    </button>
    <div class="pswp__caption">
      <div class="pswp__caption__center"></div>
    </div>
    </div>
    </div>
</div>

  

  
    
<div class="modal-dialog">
    
    <div class="modal-content">
      <div id="closeSearch" title="Close" class="close">X</div>
      <div class="modal-header">
        <div class="modal-title">Search</div>
      </div>
      <div class="modal-body">
          <script>
            (function() {
              var cx = '006410550977056989513:rg9ancg1smm';
              var gcse = document.createElement('script');
              gcse.type = 'text/javascript';
              gcse.async = true;
              gcse.src = (document.location.protocol == 'https:' ? 'https:' :
                  'http:') +
                '//cse.google.com/cse.js?cx=' + cx;
              var s = document.getElementsByTagName('script')[0];
              s.parentNode.insertBefore(gcse, s);
            })();
          </script>
          <gcse:search></gcse:search>
      </div>
    </div>
</div>

  

  

  <header id="header" class="header container">
    <div class="logo-wrapper">
  <a href="/" class="logo">
    
      BGBiao的SRE人生
    
  </a>
</div>

<nav class="site-navbar">
  <ul id="menu" class="menu">
    
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/">首页</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/post/">全部文章</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/categories/">分类</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          <a class="menu-item-link menu-parent" href="https://bgbiao.top/">学习文档</a>
          <ul class="submenu">
            
              <li>
                <a href="https://otexts.com/fppcn/index.html">&lt;预测:方法与实践&gt;</a>
              </li>
            
          </ul>

        

      </li>
    
        <li class="menu-item">
        
          
          <a class="menu-item-link menu-parent" href="https://bgbiao.top/">Golang相关</a>
          <ul class="submenu">
            
              <li>
                <a href="https://gowebexamples.com/">Golang web开发示例</a>
              </li>
            
              <li>
                <a href="http://go-database-sql.org/">Golang SQL开发教程</a>
              </li>
            
              <li>
                <a href="https://rpcx.io/">Golang RPCX框架</a>
              </li>
            
              <li>
                <a href="https://github.com/talkgo/read">Golang 夜读</a>
              </li>
            
              <li>
                <a href="https://books.studygolang.com/">Golang 开源书籍</a>
              </li>
            
          </ul>

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/devops/">DevOps/SRE</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/tags/">标签</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/books/">我的书单</a>
          

        

      </li>
    
        <li class="menu-item">
        
          
          
            <a class="menu-item-link" href="https://bgbiao.top/about/">关于我</a>
          

        

      </li>
    

    
    

    
      <li class="menu-item">
        <a id="openSearch" class="menu-item-link menu-item-search" href="#">
          <i class="iconfont">
            <svg version="1.1" viewBox="0 0 1024 1024"
  xmlns="http://www.w3.org/2000/svg" width="18" height="18"
  xmlns:xlink="http://www.w3.org/1999/xlink">
  <path d="M973.81454219 973.81454219a91.78207815 91.78207815 0 0 1-129.80999631 0l-161.97482118-161.97482118a425.48527711 425.48527711 0 0 1-230.35931791 68.16531768 428.3346319 428.3346319 0 1 1 428.3346319-428.3346319 425.48527711 425.48527711 0 0 1-68.16531768 230.35931791l162.02961656 161.97482118a91.83687354 91.83687354 0 0 1-0.05479538 129.80999631zM451.67040679 145.69361559a305.97679241 305.97679241 0 1 0 0 611.95358361 305.97679241 305.97679241 0 0 0 0-611.95358361z">
  </path>
</svg>

          </i>
        </a>
      </li>
    
  </ul>
</nav>

  </header>

  <div id="mobile-panel">
    <main id="main" class="main bg-llight">
      <div class="content-wrapper">
        <div id="content" class="content container">
          <article class="post bg-white">
    
    <header class="post-header">
      <h1 class="post-title">GPU环境下玩转Docker(二)</h1>
      
      <div class="post-meta">
        <time datetime="2017-10-26" class="post-time">
          2017-10-26
        </time>
        <div class="post-category">
            <a href="https://bgbiao.top/categories/%E8%BF%90%E7%BB%B4/"> 运维 </a>
            
          </div>
        <span class="more-meta"> 3857 words </span>
          <span class="more-meta"> 8 min read </span>

        
        

        
        
      </div>
    </header>

    
    
<div class="post-toc" id="post-toc">
  <h2 class="post-toc-title">Table of Contents</h2>
  <div class="post-toc-content">
    <nav id="TableOfContents">
  <ul>
    <li>
      <ul>
        <li><a href="#使用nvidia驱动docker容器来识别gpu环境">使用NVIDIA驱动Docker容器来识别gpu环境</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </div>
</div>

    
    <div class="post-content">
      <blockquote>
<p>前言:
在一节中<a href="https://xxbandy.github.io/2017/10/26/GPU%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%8E%A9%E8%BD%ACDocker-%E4%B8%80/"></a>,我们已经在GPU物理机上准备好了GPU环境，本篇文章介绍如何使用Docker来管理GPU容器。</p>
</blockquote>
<h3 id="使用nvidia驱动docker容器来识别gpu环境">使用NVIDIA驱动Docker容器来识别gpu环境</h3>
<p>使用Docker方式来运行GPU任务计算。英伟达(nvidia)官方提供了一个插件<a href="https://github.com/NVIDIA/nvidia-docker">NVIDIA-Docker</a>，封装docker的相关参数来绑定GPU相关信息，以给容器提供gpu环境。下面主要介绍使用<code>nvidia-docker</code>插件运行容器以共享宿主机的GPU资源，后面会捎带的讲解如何使用docker原生的方式来运行GPU容器环境。</p>
<p><a href="https://devblogs.nvidia.com/parallelforall/nvidia-docker-gpu-server-application-deployment-made-easy/">nvidia-docker部署gpu应用</a>
<a href="https://github.com/NVIDIA/nvidia-docker/wiki/Motivation">nvidia-docker的动机</a></p>
<p><a href="https://github.com/NVIDIA/nvidia-docker/wiki/Installation">nvidia插件安装指导</a>
<a href="https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker">nvidia-docker使用指南</a>
<a href="https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker-plugin">nvidia-docker插件</a></p>
<p><a href="https://github.com/NVIDIA/nvidia-docker/wiki/NVIDIA-driver">nvidia驱动</a>
<a href="https://github.com/NVIDIA/nvidia-docker/wiki/GPU-isolation">GPU隔离技术</a>
<a href="https://github.com/NVIDIA/nvidia-docker/wiki/Image-inspection">镜像检测</a></p>
<h4 id="快速部署安装">快速部署安装</h4>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># 安装 nvidia-docker and nvidia-docker-plugin
</span></span><span class="line"><span class="cl">$ wget -P /tmp https://github.com/NVIDIA/nvidia-docker/releases/download/v1.0.1/nvidia-docker-1.0.1-1.x86_64.rpm
</span></span><span class="line"><span class="cl">$ rpm -i /tmp/nvidia-docker*.rpm &amp;&amp; rm /tmp/nvidia-docker*.rpm
</span></span><span class="line"><span class="cl">$ systemctl start nvidia-docker
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 测试docker容器内部可以识别宿主机的GPU环境
</span></span><span class="line"><span class="cl"># nvidia-docker run --rm idockerhub.xxb.com/nvidia-docker/cuda8.0-runtime:centos6-17-10-19 nvidia-smi
</span></span><span class="line"><span class="cl">Thu Oct 19 08:07:09 2017
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
</span></span><span class="line"><span class="cl">|-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span class="line"><span class="cl">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
</span></span><span class="line"><span class="cl">|===============================+======================+======================|
</span></span><span class="line"><span class="cl">|   0  Tesla M40 24GB      On   | 0000:04:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   28C    P8    18W / 250W |      0MiB / 22939MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">|   1  Tesla M40           On   | 0000:05:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   31C    P8    17W / 250W |      0MiB / 11443MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">|   2  Tesla M40 24GB      On   | 0000:06:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   26C    P8    18W / 250W |      0MiB / 22939MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">|   3  Tesla M40           On   | 0000:07:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   27C    P8    16W / 250W |      0MiB / 11443MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| Processes:                                                       GPU Memory |
</span></span><span class="line"><span class="cl">|  GPU       PID  Type  Process name                               Usage      |
</span></span><span class="line"><span class="cl">|=============================================================================|
</span></span><span class="line"><span class="cl">|  No running processes found                                                 |
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># 绑定GPU核心到docker容器内部(可以看到该容器内部其实只绑定了一块宿主机上的第一块GPU卡)实现GPU隔离
</span></span><span class="line"><span class="cl"># NV_GPU=0 nvidia-docker run --rm idockerhub.xxb.com/nvidia-docker/cuda8.0-runtime:centos6-17-10-19 nvidia-smi
</span></span><span class="line"><span class="cl">Wed Oct 25 07:17:03 2017
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
</span></span><span class="line"><span class="cl">|-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span class="line"><span class="cl">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
</span></span><span class="line"><span class="cl">|===============================+======================+======================|
</span></span><span class="line"><span class="cl">|   0  Tesla M40 24GB      On   | 0000:04:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   23C    P8    17W / 250W |      0MiB / 22939MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| Processes:                                                       GPU Memory |
</span></span><span class="line"><span class="cl">|  GPU       PID  Type  Process name                               Usage      |
</span></span><span class="line"><span class="cl">|=============================================================================|
</span></span><span class="line"><span class="cl">|  No running processes found                                                 |
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># NV_GPU=&#34;0,2&#34; nvidia-docker run --rm -ti idockerhub.xxb.com/nvidia-docker/cuda8.0-runtime:centos6-17-10-19 nvidia-smi
</span></span><span class="line"><span class="cl">Wed Oct 25 08:37:25 2017
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| NVIDIA-SMI 375.39                 Driver Version: 375.39                    |
</span></span><span class="line"><span class="cl">|-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |
</span></span><span class="line"><span class="cl">| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |
</span></span><span class="line"><span class="cl">|===============================+======================+======================|
</span></span><span class="line"><span class="cl">|   0  Tesla M40 24GB      On   | 0000:04:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   34C    P0    59W / 250W |  21802MiB / 22939MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">|   1  Tesla M40 24GB      On   | 0000:06:00.0     Off |                    0 |
</span></span><span class="line"><span class="cl">| N/A   34C    P0    57W / 250W |  21800MiB / 22939MiB |      0%      Default |
</span></span><span class="line"><span class="cl">+-------------------------------+----------------------+----------------------+
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span><span class="line"><span class="cl">| Processes:                                                       GPU Memory |
</span></span><span class="line"><span class="cl">|  GPU       PID  Type  Process name                               Usage      |
</span></span><span class="line"><span class="cl">|=============================================================================|
</span></span><span class="line"><span class="cl">+-----------------------------------------------------------------------------+
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>注意1：上述输出的表格中可用看到，使用nvidia-docker工具创建的容器内部其实是可以识别到宿主机的GPU设备的</code></p>
<p><code>注意2：CPU和GPU其实很类似，无法智能识别需要几块卡设备，只能通过NV_GPU方式来给容器绑定GPU卡(类似于cpu set的方式)；需要管理员动态的调整需要分配的具体GPU卡设备</code></p>
<h4 id="容器内部gpu环境验证">容器内部GPU环境验证</h4>
<p>2.1 <strong>使用官方的gpu版本的tensorflow镜像测试GPU设备</strong></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># nvidia-docker run -it --rm -v /usr/lib64/libcuda.so.1:/usr/local/nvidia/lib64/libcuda.so.1 idockerhub.xxb.com/jdjr/tensorflow-gpu:17-10-17 bash
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">root@6b4ad215279e:/notebooks# python
</span></span><span class="line"><span class="cl">Python 2.7.12 (default, Nov 19 2016, 06:48:10)
</span></span><span class="line"><span class="cl">[GCC 5.4.0 20160609] on linux2
</span></span><span class="line"><span class="cl">Type &#34;help&#34;, &#34;copyright&#34;, &#34;credits&#34; or &#34;license&#34; for more information.
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; import tensorflow as tf
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; a = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[2, 3], name=&#39;a&#39;)
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; b = tf.constant([1.0, 2.0, 3.0, 4.0, 5.0, 6.0], shape=[3, 2], name=&#39;b&#39;)
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; c = tf.matmul(a, b)
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; sess = tf.Session(config=tf.ConfigProto(log_device_placement=True))
</span></span><span class="line"><span class="cl">2017-10-19 08:01:39.862500: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:39.862600: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:39.862646: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:39.862676: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:39.862711: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:40.388656: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40 24GB
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:04:00.0
</span></span><span class="line"><span class="cl">Total memory: 22.40GiB
</span></span><span class="line"><span class="cl">Free memory: 22.29GiB
</span></span><span class="line"><span class="cl">2017-10-19 08:01:40.682810: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x1e2dac0 exists before initializing the StreamExecutor. We haven&#39;t verified StreamExecutor works with that.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:40.684222: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:05:00.0
</span></span><span class="line"><span class="cl">Total memory: 11.17GiB
</span></span><span class="line"><span class="cl">Free memory: 11.07GiB
</span></span><span class="line"><span class="cl">2017-10-19 08:01:40.995170: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x329a280 exists before initializing the StreamExecutor. We haven&#39;t verified StreamExecutor works with that.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:40.998560: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40 24GB
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:06:00.0
</span></span><span class="line"><span class="cl">Total memory: 22.40GiB
</span></span><span class="line"><span class="cl">Free memory: 22.29GiB
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.289133: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x329dc00 exists before initializing the StreamExecutor. We haven&#39;t verified StreamExecutor works with that.
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.290444: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:07:00.0
</span></span><span class="line"><span class="cl">Total memory: 11.17GiB
</span></span><span class="line"><span class="cl">Free memory: 11.07GiB
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294062: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294093: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294156: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294178: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294215: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0)
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294229: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla M40, pci bus id: 0000:05:00.0)
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294239: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla M40 24GB, pci bus id: 0000:06:00.0)
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.294248: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla M40, pci bus id: 0000:07:00.0)
</span></span><span class="line"><span class="cl">Device mapping:
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:1 -&gt; device: 1, name: Tesla M40, pci bus id: 0000:05:00.0
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:2 -&gt; device: 2, name: Tesla M40 24GB, pci bus id: 0000:06:00.0
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:3 -&gt; device: 3, name: Tesla M40, pci bus id: 0000:07:00.0
</span></span><span class="line"><span class="cl">2017-10-19 08:01:41.875931: I tensorflow/core/common_runtime/direct_session.cc:300] Device mapping:
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:0 -&gt; device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:1 -&gt; device: 1, name: Tesla M40, pci bus id: 0000:05:00.0
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:2 -&gt; device: 2, name: Tesla M40 24GB, pci bus id: 0000:06:00.0
</span></span><span class="line"><span class="cl">/job:localhost/replica:0/task:0/gpu:3 -&gt; device: 3, name: Tesla M40, pci bus id: 0000:07:00.0
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&gt;&gt;&gt; print(sess.run(c))
</span></span><span class="line"><span class="cl">MatMul: (MatMul): /job:localhost/replica:0/task:0/gpu:0
</span></span><span class="line"><span class="cl">2017-10-19 08:01:51.333248: I tensorflow/core/common_runtime/simple_placer.cc:872] MatMul: (MatMul)/job:localhost/replica:0/task:0/gpu:0
</span></span><span class="line"><span class="cl">b: (Const): /job:localhost/replica:0/task:0/gpu:0
</span></span><span class="line"><span class="cl">2017-10-19 08:01:51.333346: I tensorflow/core/common_runtime/simple_placer.cc:872] b: (Const)/job:localhost/replica:0/task:0/gpu:0
</span></span><span class="line"><span class="cl">a: (Const): /job:localhost/replica:0/task:0/gpu:0
</span></span><span class="line"><span class="cl">2017-10-19 08:01:51.333408: I tensorflow/core/common_runtime/simple_placer.cc:872] a: (Const)/job:localhost/replica:0/task:0/gpu:0
</span></span><span class="line"><span class="cl">[[ 22.  28.]
</span></span><span class="line"><span class="cl"> [ 49.  64.]]
</span></span><span class="line"><span class="cl">&gt;&gt;&gt;
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>注意：由以上完整输出可以看到当前的容器环境内部可以正常识别到GPU设备，并可以进行GPU计算</code></p>
<p>2.1 <strong>使用TensorFlow-gpu环境</strong></p>
<p>官方<code>tensorflow/tensorflow:latest-gpu</code>镜像是使用<code>jupyter</code>环境来提供一个可编程的<code>TensorFlow</code>环境。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># nvidia-docker run -itd -p 8888:8888 -p 6006:6006 idockerhub.xxb.com/jdjr/tensorflow-gpu:17-10-17
</span></span><span class="line"><span class="cl"># docker logs -f evil_ride
</span></span><span class="line"><span class="cl">[I 07:24:37.730 NotebookApp] Writing notebook server cookie secret to /root/.local/share/jupyter/runtime/notebook_cookie_secret
</span></span><span class="line"><span class="cl">[W 07:24:37.742 NotebookApp] WARNING: The notebook server is listening on all IP addresses and not using encryption. This is not recommended.
</span></span><span class="line"><span class="cl">[I 07:24:37.747 NotebookApp] Serving notebooks from local directory: /notebooks
</span></span><span class="line"><span class="cl">[I 07:24:37.747 NotebookApp] 0 active kernels
</span></span><span class="line"><span class="cl">[I 07:24:37.747 NotebookApp] The Jupyter Notebook is running at: http://[all ip addresses on your system]:8888/?token=b4f66281d6b1f89bd6fda85c6e88a022ef6d38308e7f284b
</span></span><span class="line"><span class="cl">[I 07:24:37.748 NotebookApp] Use Control-C to stop this server and shut down all kernels (twice to skip confirmation).
</span></span><span class="line"><span class="cl">[C 07:24:37.748 NotebookApp]
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">    Copy/paste this URL into your browser when you connect for the first time,
</span></span><span class="line"><span class="cl">    to login with a token:
</span></span><span class="line"><span class="cl">        http://localhost:8888/?token=b4f66281d6b1f89bd6fda85c6e88a022ef6d38308e7f284b
</span></span></code></pre></td></tr></table>
</div>
</div><p>成功运行后如上述日志显示，直接访问上述链接，或者访问本机的8888端口，并将后面的token作为密码输入后即可登录jupyter环境，使用tensorflow环境。</p>
<p><img src="http://oyep1jupk.bkt.clouddn.com/docker/nvidia-docker/tensorflow-jupyter.png" alt=""></p>
<p>点击右上角<code>New</code>下面的<code>Python2</code>可进入jupyter的交互式编程环境。并进行GPU简单程序的测试，测试结果如下：
<img src="http://oyep1jupk.bkt.clouddn.com/docker/nvidia-docker/tensorflow-gpu.png" alt=""></p>
<p>可以看到，使用TensorFlow官方的镜像文件，可以直接使用jupyter进行运行gpu计算任务。该任务运行完毕之后，运行过程中的日志信息如下：</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># docker logs -f evil_ride
</span></span><span class="line"><span class="cl">[I 07:37:01.806 NotebookApp] Adapting to protocol v5.1 for kernel 253b4ef2-b573-4256-a14b-6a840777923d
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.174144: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.1 instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.174295: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use SSE4.2 instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.174320: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use AVX instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.174342: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use AVX2 instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.174366: W tensorflow/core/platform/cpu_feature_guard.cc:45] The TensorFlow library wasn&#39;t compiled to use FMA instructions, but these are available on your machine and could speed up CPU computations.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.719139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 0 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40 24GB
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:04:00.0
</span></span><span class="line"><span class="cl">Total memory: 22.40GiB
</span></span><span class="line"><span class="cl">Free memory: 22.29GiB
</span></span><span class="line"><span class="cl">2017-10-25 07:37:41.999138: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x1d90ab0 exists before initializing the StreamExecutor. We haven&#39;t verified StreamExecutor works with that.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.001131: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 1 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:05:00.0
</span></span><span class="line"><span class="cl">Total memory: 11.17GiB
</span></span><span class="line"><span class="cl">Free memory: 11.07GiB
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.317011: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x3acbf10 exists before initializing the StreamExecutor. We haven&#39;t verified StreamExecutor works with that.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.318986: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 2 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40 24GB
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:06:00.0
</span></span><span class="line"><span class="cl">Total memory: 22.40GiB
</span></span><span class="line"><span class="cl">Free memory: 22.29GiB
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.612283: W tensorflow/stream_executor/cuda/cuda_driver.cc:523] A non-primary context 0x3acf890 exists before initializing the StreamExecutor. We haven&#39;t verified StreamExecutor works with that.
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.614320: I tensorflow/core/common_runtime/gpu/gpu_device.cc:955] Found device 3 with properties:
</span></span><span class="line"><span class="cl">name: Tesla M40
</span></span><span class="line"><span class="cl">major: 5 minor: 2 memoryClockRate (GHz) 1.112
</span></span><span class="line"><span class="cl">pciBusID 0000:07:00.0
</span></span><span class="line"><span class="cl">Total memory: 11.17GiB
</span></span><span class="line"><span class="cl">Free memory: 11.07GiB
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619214: I tensorflow/core/common_runtime/gpu/gpu_device.cc:976] DMA: 0 1 2 3
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619252: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 0:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619270: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 1:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 2:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619351: I tensorflow/core/common_runtime/gpu/gpu_device.cc:986] 3:   Y Y Y Y
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619386: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619410: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla M40, pci bus id: 0000:05:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619431: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla M40 24GB, pci bus id: 0000:06:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:42.619464: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla M40, pci bus id: 0000:07:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:56.406493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:0) -&gt; (device: 0, name: Tesla M40 24GB, pci bus id: 0000:04:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:56.406573: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:1) -&gt; (device: 1, name: Tesla M40, pci bus id: 0000:05:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:56.406607: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:2) -&gt; (device: 2, name: Tesla M40 24GB, pci bus id: 0000:06:00.0)
</span></span><span class="line"><span class="cl">2017-10-25 07:37:56.406627: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1045] Creating TensorFlow device (/gpu:3) -&gt; (device: 3, name: Tesla M40, pci bus id: 0000:07:00.0)
</span></span><span class="line"><span class="cl">[I 07:39:01.292 NotebookApp] Saving file at /Untitled1.ipynb
</span></span></code></pre></td></tr></table>
</div>
</div><p>由上述日志输出可以看到该任务其实是占用宿主机的4个GPU卡进行任务计算的。</p>
<p><code>注意：因为jupyter是通过websocket和python环境进行交互的，如果需要使用反向代理之类的工具访问服务，需要反向代理工具支持websocket协议</code>。在实际部署过程中，我们使用Nginx进行反向代理，Nginx默认是在1.3版本开始支持websocket的，如果nginx版本大于1.3只需要对相应的配置文件中增加如下内容即可。</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl">upstream tomcat_txxb.wp.com {
</span></span><span class="line"><span class="cl">        server 10.0.0.10:8888  weight=10 max_fails=2 fail_timeout=30s;
</span></span><span class="line"><span class="cl">                }
</span></span><span class="line"><span class="cl">location / {
</span></span><span class="line"><span class="cl">        proxy_next_upstream     http_500 http_502 http_503 http_504 error timeout invalid_header;
</span></span><span class="line"><span class="cl">        proxy_set_header        Host  $host;
</span></span><span class="line"><span class="cl">        proxy_set_header        X-Forwarded-For $proxy_add_x_forwarded_for;
</span></span><span class="line"><span class="cl">        proxy_pass              http://tomcat_txxb.wp.com;
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">location ~ /api/kernels/ {
</span></span><span class="line"><span class="cl">        proxy_pass            http://tomcat_txxb.wp.com;
</span></span><span class="line"><span class="cl">        proxy_set_header      Host $host;
</span></span><span class="line"><span class="cl">        # websocket support
</span></span><span class="line"><span class="cl">        proxy_http_version    1.1;
</span></span><span class="line"><span class="cl">        proxy_set_header      Upgrade &#34;websocket&#34;;
</span></span><span class="line"><span class="cl">        proxy_set_header      Connection &#34;Upgrade&#34;;
</span></span><span class="line"><span class="cl">        proxy_read_timeout    86400;
</span></span><span class="line"><span class="cl">    }
</span></span><span class="line"><span class="cl">location ~ /terminals/ {
</span></span><span class="line"><span class="cl">        proxy_pass            http://tomcat_txxb.wp.com;
</span></span><span class="line"><span class="cl">        proxy_set_header      Host $host;
</span></span><span class="line"><span class="cl">        # websocket support
</span></span><span class="line"><span class="cl">        proxy_http_version    1.1;
</span></span><span class="line"><span class="cl">        proxy_set_header      Upgrade &#34;websocket&#34;;
</span></span><span class="line"><span class="cl">        proxy_set_header      Connection &#34;Upgrade&#34;;
</span></span><span class="line"><span class="cl">        proxy_read_timeout    86400;
</span></span><span class="line"><span class="cl">}
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>使用docker原生方式运行GPU容器环境(无非就是指定设备挂载或者将给容器开放特权来操作宿主机硬件)</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span><span class="lnt">3
</span><span class="lnt">4
</span><span class="lnt">5
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># export DEVICES=$(\ls /dev/nvidia* | xargs -I{} echo &#39;--device {}:{}&#39;)
</span></span><span class="line"><span class="cl"># docker run  $DEVICES -it --rm -v /usr/lib64/libcuda.so.1:/usr/local/nvidia/lib64/libcuda.so.1 -v /usr/lib64/libnvidia-fatbinaryloader.so.375.39:/usr/local/nvidia/lib64/libnvidia-fatbinaryloader.so.375.39  -v /root/gpu-example/:/tmp idockerhub.xxb.com/jdjr/tensorflow-gpu:17-10-17 bash
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl"># docker run  --privileged -it --rm -v /usr/lib64/libcuda.so.1:/usr/local/nvidia/lib64/libcuda.so.1 -v /usr/lib64/libnvidia-fatbinaryloader.so.375.39:/usr/local/nvidia/lib64/libnvidia-fatbinaryloader.so.375.39  -v /root/gpu-example/:/tmp idockerhub.xxb.com/jdjr/tensorflow-gpu:17-10-17 bash
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>注意：当然在上述tensorflow测试过程中，只需要部分的cuda依赖库，在很多其他的业务场景下可能需要其他的依赖库，建议使用如下方式挂载全部库依赖</code></p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># export CUDA_SO=&#34;$(\ls /usr/lib64/libcuda* | xargs -I{} echo &#39;-v  {}:{}&#39;)  $(\ls /usr/lib64/libnvidia* | xargs -I{} echo &#39;-v {}:{}&#39;)&#34;
</span></span><span class="line"><span class="cl"># docker run $DEVICES $CUDA_SO --rm idockerhub.xxb.com/jdjr/tensorflow-gpu:17-10-17 bash
</span></span></code></pre></td></tr></table>
</div>
</div><blockquote>
<p>思考：其实从nvidia-docker的运行方式和docker原生的运行方式上来看，nvidia其实是使用自己的插件来封装了一些docker的默认参数，比如说上述的GPU设备，以及相关lib库依赖。从<code>nvidia-docker</code>的底层实现上也可以看得出来</p>
</blockquote>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt">1
</span><span class="lnt">2
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># curl -s http://localhost:3476/docker/cli
</span></span><span class="line"><span class="cl">--volume-driver=nvidia-docker --volume=nvidia_driver_375.39:/usr/local/nvidia:ro --device=/dev/nvidiactl --device=/dev/nvidia-uvm --device=/dev/nvidia-uvm-tools --device=/dev/nvidia0 --device=/dev/nvidia1 --device=/dev/nvidia2 --device=/dev/nvidia3
</span></span></code></pre></td></tr></table>
</div>
</div><p>也就是说<code>nvidia-docker</code>在创建容器的时候，默认是加入了上述的参数的。也就是docker原生调用过程中使用的<code>-v</code>和<code>--device</code>.其实可以查看<code>nvidia-docker</code>工具生成的容器的相关配置:</p>
<div class="highlight"><div class="chroma">
<table class="lntable"><tr><td class="lntd">
<pre tabindex="0" class="chroma"><code><span class="lnt"> 1
</span><span class="lnt"> 2
</span><span class="lnt"> 3
</span><span class="lnt"> 4
</span><span class="lnt"> 5
</span><span class="lnt"> 6
</span><span class="lnt"> 7
</span><span class="lnt"> 8
</span><span class="lnt"> 9
</span><span class="lnt">10
</span><span class="lnt">11
</span><span class="lnt">12
</span><span class="lnt">13
</span><span class="lnt">14
</span><span class="lnt">15
</span><span class="lnt">16
</span><span class="lnt">17
</span><span class="lnt">18
</span><span class="lnt">19
</span><span class="lnt">20
</span><span class="lnt">21
</span><span class="lnt">22
</span><span class="lnt">23
</span><span class="lnt">24
</span><span class="lnt">25
</span><span class="lnt">26
</span><span class="lnt">27
</span><span class="lnt">28
</span><span class="lnt">29
</span><span class="lnt">30
</span><span class="lnt">31
</span><span class="lnt">32
</span><span class="lnt">33
</span><span class="lnt">34
</span><span class="lnt">35
</span><span class="lnt">36
</span><span class="lnt">37
</span><span class="lnt">38
</span><span class="lnt">39
</span><span class="lnt">40
</span><span class="lnt">41
</span><span class="lnt">42
</span><span class="lnt">43
</span><span class="lnt">44
</span><span class="lnt">45
</span><span class="lnt">46
</span><span class="lnt">47
</span><span class="lnt">48
</span><span class="lnt">49
</span><span class="lnt">50
</span><span class="lnt">51
</span><span class="lnt">52
</span><span class="lnt">53
</span><span class="lnt">54
</span><span class="lnt">55
</span><span class="lnt">56
</span><span class="lnt">57
</span><span class="lnt">58
</span><span class="lnt">59
</span><span class="lnt">60
</span><span class="lnt">61
</span><span class="lnt">62
</span><span class="lnt">63
</span><span class="lnt">64
</span><span class="lnt">65
</span><span class="lnt">66
</span><span class="lnt">67
</span><span class="lnt">68
</span><span class="lnt">69
</span><span class="lnt">70
</span><span class="lnt">71
</span><span class="lnt">72
</span><span class="lnt">73
</span><span class="lnt">74
</span><span class="lnt">75
</span><span class="lnt">76
</span><span class="lnt">77
</span><span class="lnt">78
</span><span class="lnt">79
</span><span class="lnt">80
</span><span class="lnt">81
</span></code></pre></td>
<td class="lntd">
<pre tabindex="0" class="chroma"><code class="language-fallback" data-lang="fallback"><span class="line"><span class="cl"># docker inspect evil_ride
</span></span><span class="line"><span class="cl">&#34;HostConfig&#34;: {
</span></span><span class="line"><span class="cl">            &#34;Binds&#34;: [
</span></span><span class="line"><span class="cl">                &#34;nvidia_driver_375.39:/usr/local/nvidia:ro&#34;
</span></span><span class="line"><span class="cl">            ],
</span></span><span class="line"><span class="cl">            ........
</span></span><span class="line"><span class="cl">}
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">&#34;Devices&#34;: [
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidiactl&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidiactl&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                },
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidia-uvm&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidia-uvm&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                },
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidia-uvm-tools&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidia-uvm-tools&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                },
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidia0&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidia0&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                },
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidia1&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidia1&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                },
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidia2&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidia2&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                },
</span></span><span class="line"><span class="cl">                {
</span></span><span class="line"><span class="cl">                    &#34;PathOnHost&#34;: &#34;/dev/nvidia3&#34;,
</span></span><span class="line"><span class="cl">                    &#34;PathInContainer&#34;: &#34;/dev/nvidia3&#34;,
</span></span><span class="line"><span class="cl">                    &#34;CgroupPermissions&#34;: &#34;rwm&#34;
</span></span><span class="line"><span class="cl">                }
</span></span><span class="line"><span class="cl">            ],
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        &#34;Mounts&#34;: [
</span></span><span class="line"><span class="cl">            {
</span></span><span class="line"><span class="cl">                &#34;Name&#34;: &#34;nvidia_driver_375.39&#34;,
</span></span><span class="line"><span class="cl">                &#34;Source&#34;: &#34;/var/lib/nvidia-docker/volumes/nvidia_driver/375.39&#34;,
</span></span><span class="line"><span class="cl">                &#34;Destination&#34;: &#34;/usr/local/nvidia&#34;,
</span></span><span class="line"><span class="cl">                &#34;Driver&#34;: &#34;nvidia-docker&#34;,
</span></span><span class="line"><span class="cl">                &#34;Mode&#34;: &#34;ro&#34;,
</span></span><span class="line"><span class="cl">                &#34;RW&#34;: false,
</span></span><span class="line"><span class="cl">                &#34;Propagation&#34;: &#34;rprivate&#34;
</span></span><span class="line"><span class="cl">            }
</span></span><span class="line"><span class="cl">        ],
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        &#34;Env&#34;: [
</span></span><span class="line"><span class="cl">                &#34;PATH=/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin&#34;,
</span></span><span class="line"><span class="cl">                &#34;CUDA_VERSION=8.0.61&#34;,
</span></span><span class="line"><span class="cl">                &#34;NVIDIA_CUDA_VERSION=8.0.61&#34;,
</span></span><span class="line"><span class="cl">                &#34;CUDA_PKG_VERSION=8-0=8.0.61-1&#34;,
</span></span><span class="line"><span class="cl">                &#34;LD_LIBRARY_PATH=/usr/local/cuda/extras/CUPTI/lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64&#34;,
</span></span><span class="line"><span class="cl">                &#34;NVIDIA_VISIBLE_DEVICES=all&#34;,
</span></span><span class="line"><span class="cl">                &#34;NVIDIA_DRIVER_CAPABILITIES=compute,utility&#34;,
</span></span><span class="line"><span class="cl">                &#34;LIBRARY_PATH=/usr/local/cuda/lib64/stubs:&#34;,
</span></span><span class="line"><span class="cl">                &#34;CUDNN_VERSION=6.0.21&#34;
</span></span><span class="line"><span class="cl">            ],
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">
</span></span><span class="line"><span class="cl">        &#34;Labels&#34;: {
</span></span><span class="line"><span class="cl">                &#34;com.nvidia.build.id&#34;: &#34;29071360&#34;,
</span></span><span class="line"><span class="cl">                &#34;com.nvidia.build.ref&#34;: &#34;836d5387f8888c3924aff7a011f9b2cd9956d3db&#34;,
</span></span><span class="line"><span class="cl">                &#34;com.nvidia.cuda.version&#34;: &#34;8.0.61&#34;,
</span></span><span class="line"><span class="cl">                &#34;com.nvidia.cudnn.version&#34;: &#34;6.0.21&#34;,
</span></span><span class="line"><span class="cl">                &#34;com.nvidia.volumes.needed&#34;: &#34;nvidia_driver&#34;,
</span></span><span class="line"><span class="cl">                &#34;maintainer&#34;: &#34;NVIDIA CORPORATION \u003ccudatools@nvidia.com\u003e&#34;
</span></span><span class="line"><span class="cl">            }
</span></span></code></pre></td></tr></table>
</div>
</div><p><code>注意:nvidia-driver的版本以及GPU型号以及nvidia-docker的版本三者是有强关联关系的</code></p>
<table>
<thead>
<tr>
<th>GPU</th>
<th>nvidia-driver</th>
<th>nvidia-docker</th>
</tr>
</thead>
<tbody>
<tr>
<td>Tesla M40</td>
<td>NVIDIA-Linux-x86_64-375.39.run</td>
<td>nvidia-docker-1.0.1-1.x86_64</td>
</tr>
<tr>
<td>P40</td>
<td>NVIDIA-Linux-x86_64-384.81.run</td>
<td>XX</td>
</tr>
</tbody>
</table>
    </div>

    
    
<div class="post-copyright">
  <p class="copyright-item">
    <span class="item-title">Author</span>
    <span class="item-content">BGBiao</span>
  </p>
  <p class="copyright-item">
    <span class="item-title">LastMod</span>
    <span class="item-content">
      2020-11-01
      
        
        
      
    </span>
  </p>
  
  <p class="copyright-item">
    <span class="item-title">License</span>
    <span class="item-content">原创文章，如需转载请注明文章作者和出处<BGBiao>。谢谢！</span>
  </p>
</div>


    
    
<div class="post-reward">
  <input type="checkbox" name="reward" id="reward" hidden />
  <label class="reward-button" for="reward">Reward</label>
  <div class="qr-code">
    
    
      <label class="qr-code-image" for="reward">
        <img class="image" src="/reward_wechat.png">
        <span>Wechat</span>
      </label>
    
      <label class="qr-code-image" for="reward">
        <img class="image" src="/reward_wechat.png">
        <span>Alipay</span>
      </label>
  </div>
</div>

    <footer class="post-footer">
      <div class="post-tags">
          <a href="https://bgbiao.top/tags/docker/">Docker</a>
          <a href="https://bgbiao.top/tags/gpu/">GPU</a>
          <a href="https://bgbiao.top/tags/nvidia/">NVIDIA</a>
          <a href="https://bgbiao.top/tags/nvidia-docker/">NVIDIA-Docker</a>
          
        </div>

      
      <nav class="post-nav">
        
          <a class="prev" href="/post/manager-your-gpus/">
            
            <i class="iconfont">
              <svg  class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M691.908486 949.511495l75.369571-89.491197c10.963703-12.998035 10.285251-32.864502-1.499144-44.378743L479.499795 515.267417 757.434875 204.940602c11.338233-12.190647 11.035334-32.285311-0.638543-44.850487l-80.46666-86.564541c-11.680017-12.583596-30.356378-12.893658-41.662889-0.716314L257.233596 494.235404c-11.332093 12.183484-11.041474 32.266891 0.657986 44.844348l80.46666 86.564541c1.772366 1.910513 3.706415 3.533476 5.750981 4.877077l306.620399 321.703933C662.505829 963.726242 680.945807 962.528973 691.908486 949.511495z"></path>
</svg>

            </i>
            <span class="prev-text nav-default">如何优雅的管理你的GPU</span>
            <span class="prev-text nav-mobile">Prev</span>
          </a>
        
          <a class="next" href="/post/gpu%E7%8E%AF%E5%A2%83%E4%B8%8B%E7%8E%A9%E8%BD%ACdocker-%E4%B8%80/">
            <span class="next-text nav-default">GPU环境下玩转Docker(一)</span>
            <span class="prev-text nav-mobile">Next</span>
            
            <i class="iconfont">
              <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="18" height="18">
  <path d="M332.091514 74.487481l-75.369571 89.491197c-10.963703 12.998035-10.285251 32.864502 1.499144 44.378743l286.278095 300.375162L266.565125 819.058374c-11.338233 12.190647-11.035334 32.285311 0.638543 44.850487l80.46666 86.564541c11.680017 12.583596 30.356378 12.893658 41.662889 0.716314l377.434212-421.426145c11.332093-12.183484 11.041474-32.266891-0.657986-44.844348l-80.46666-86.564541c-1.772366-1.910513-3.706415-3.533476-5.750981-4.877077L373.270379 71.774697C361.493148 60.273758 343.054193 61.470003 332.091514 74.487481z"></path>
</svg>

            </i>
          </a>
      </nav>
    </footer>
  </article>

  
  

  
  

  

  
  
    <div class="post bg-white">
      <script src="https://utteranc.es/client.js"
            repo= "BGBiao/comments-bgbiao.top"
            issue-term="pathname"
            theme="github-dark"
            crossorigin="anonymous"
            async>
      </script>
    </div>
  

  

  

  

    

  

        </div>
      </div>
    </main>

    <footer id="footer" class="footer">
      <div class="icon-links">
  
  
    <a href="mailto:weichuangxxb@qq.com" rel="me noopener" class="iconfont"
      title="email" >
      <svg class="icon" viewBox="0 0 1451 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M664.781909 681.472759 0 97.881301C0 3.997201 71.046997 0 71.046997 0L474.477909 0 961.649408 0 1361.641813 0C1361.641813 0 1432.688811 3.997201 1432.688811 97.881301L771.345323 681.472759C771.345323 681.472759 764.482731 685.154773 753.594283 688.65053L753.594283 688.664858C741.602731 693.493018 729.424896 695.068979 718.077952 694.839748 706.731093 695.068979 694.553173 693.493018 682.561621 688.664858L682.561621 688.65053C671.644501 685.140446 664.781909 681.472759 664.781909 681.472759L664.781909 681.472759ZM718.063616 811.603883C693.779541 811.016482 658.879232 802.205449 619.10784 767.734955 542.989056 701.759633 0 212.052267 0 212.052267L0 942.809523C0 942.809523 0 1024 83.726336 1024L682.532949 1024 753.579947 1024 1348.948139 1024C1432.688811 1024 1432.688811 942.809523 1432.688811 942.809523L1432.688811 212.052267C1432.688811 212.052267 893.138176 701.759633 817.019477 767.734955 777.248 802.205449 742.347691 811.03081 718.063616 811.603883L718.063616 811.603883Z"></path>
</svg>

    </a>
  
    <a href="https://github.com/BGBiao/" rel="me noopener" class="iconfont"
      title="github"  target="_blank"
      >
      <svg class="icon" style="" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M512 12.672c-282.88 0-512 229.248-512 512 0 226.261333 146.688 418.133333 350.08 485.76 25.6 4.821333 34.986667-11.008 34.986667-24.618667 0-12.16-0.426667-44.373333-0.64-87.04-142.421333 30.890667-172.458667-68.693333-172.458667-68.693333C188.672 770.986667 155.008 755.2 155.008 755.2c-46.378667-31.744 3.584-31.104 3.584-31.104 51.413333 3.584 78.421333 52.736 78.421333 52.736 45.653333 78.293333 119.850667 55.68 149.12 42.581333 4.608-33.109333 17.792-55.68 32.426667-68.48-113.706667-12.8-233.216-56.832-233.216-253.013333 0-55.893333 19.84-101.546667 52.693333-137.386667-5.76-12.928-23.04-64.981333 4.48-135.509333 0 0 42.88-13.738667 140.8 52.48 40.96-11.392 84.48-17.024 128-17.28 43.52 0.256 87.04 5.888 128 17.28 97.28-66.218667 140.16-52.48 140.16-52.48 27.52 70.528 10.24 122.581333 5.12 135.509333 32.64 35.84 52.48 81.493333 52.48 137.386667 0 196.693333-119.68 240-233.6 252.586667 17.92 15.36 34.56 46.762667 34.56 94.72 0 68.522667-0.64 123.562667-0.64 140.202666 0 13.44 8.96 29.44 35.2 24.32C877.44 942.592 1024 750.592 1024 524.672c0-282.752-229.248-512-512-512"></path>
</svg>

    </a>
  
    <a href="https://www.zhihu.com/people/bgbiao" rel="me noopener" class="iconfont"
      title="zhihu"  target="_blank"
      >
      <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="36" height="36">
  <path d="M351.791182 562.469462l192.945407 0c0-45.367257-21.3871-71.939449-21.3871-71.939449L355.897709 490.530013c3.977591-82.182744 7.541767-187.659007 8.816806-226.835262l159.282726 0c0 0-0.86367-67.402109-18.578124-67.402109s-279.979646 0-279.979646 0 16.850783-88.141456 39.318494-127.053698c0 0-83.60514-4.510734-112.121614 106.962104S81.344656 355.077018 76.80834 367.390461c-4.536316 12.313443 24.62791 5.832845 36.941354 0 12.313443-5.832845 68.050885-25.924439 84.252893-103.69571l86.570681 0c1.165546 49.28652 4.596691 200.335724 3.515057 226.835262L109.86113 490.530013c-25.275663 18.147312-33.701566 71.939449-33.701566 71.939449L279.868105 562.469462c-8.497535 56.255235-23.417339 128.763642-44.275389 167.210279-33.05279 60.921511-50.55235 116.65793-169.802314 212.576513 0 0-19.442818 14.257725 40.829917 9.073656 60.273758-5.185093 117.305683-20.739347 156.840094-99.807147 20.553105-41.107233 41.805128-93.250824 58.386782-146.138358l-0.055259 0.185218 167.855986 193.263655c0 0 22.035876-51.847855 5.832845-108.880803L371.045711 650.610918l-42.1244 31.157627-0.045025 0.151449c11.69946-41.020252 20.11206-81.5749 22.726607-116.858498C351.665315 564.212152 351.72876 563.345412 351.791182 562.469462z"></path>
  <path d="M584.918753 182.033893l0 668.840094 70.318532 0 28.807093 80.512708 121.875768-80.512708 153.600307 0L959.520453 182.033893 584.918753 182.033893zM887.150192 778.934538l-79.837326 0-99.578949 65.782216-23.537066-65.782216-24.855084 0L659.341766 256.673847l227.807403 0L887.149169 778.934538z"></path>
</svg>

    </a>
  
    <a href="https://juejin.cn/user/1556564193589431" rel="me noopener" class="iconfont"
      title="juejin"  target="_blank"
      >
      <svg width="36" height="28" viewBox="0 0 36 28" fill="none" xmlns="http://www.w3.org/2000/svg">
<path fill-rule="evenodd" clip-rule="evenodd" d="M17.5875 6.77268L21.8232 3.40505L17.5875 0.00748237L17.5837 0L13.3555 3.39757L17.5837 6.76894L17.5875 6.77268ZM17.5863 17.3955H17.59L28.5161 8.77432L25.5526 6.39453L17.59 12.6808H17.5863L17.5825 12.6845L9.61993 6.40201L6.66016 8.78181L17.5825 17.3992L17.5863 17.3955ZM17.5828 23.2891L17.5865 23.2854L32.2133 11.7456L35.1768 14.1254L28.5238 19.3752L17.5865 28L0.284376 14.3574L0 14.1291L2.95977 11.7531L17.5828 23.2891Z" fill="#1E80FF"/>
</svg>

    </a>


<a href="https://bgbiao.top/index.xml" rel="noopener alternate" type="application/rss&#43;xml"
    class="iconfont" title="rss" target="_blank">
    <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="30" height="30">
  <path d="M819.157333 1024C819.157333 574.592 449.408 204.8 0 204.8V0c561.706667 0 1024 462.293333 1024 1024h-204.842667zM140.416 743.04a140.8 140.8 0 0 1 140.501333 140.586667A140.928 140.928 0 0 1 140.074667 1024C62.72 1024 0 961.109333 0 883.626667s62.933333-140.544 140.416-140.586667zM678.784 1024h-199.04c0-263.210667-216.533333-479.786667-479.744-479.786667V345.173333c372.352 0 678.784 306.517333 678.784 678.826667z"></path>
</svg>

  </a>
   
</div>

<div class="copyright">
  <span class="power-by">
    Powered by <a class="hexo-link" href="https://gohugo.io">Hugo</a>
  </span>
  <span class="division">|</span>
  <span class="theme-info">
    Theme - <a class="theme-link" href="https://github.com/xianmin/hugo-theme-jane">Jane</a>
  </span>

  <span class="copyright-year">
    &copy;
    
      2017 -
    2022
    <span class="heart">
      
      <i class="iconfont">
        <svg class="icon" viewBox="0 0 1025 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="14" height="14">
  <path d="M1000.1 247.9c-15.5-37.3-37.6-70.6-65.7-98.9-54.4-54.8-125.8-85-201-85-85.7 0-166 39-221.4 107.4C456.6 103 376.3 64 290.6 64c-75.1 0-146.5 30.4-201.1 85.6-28.2 28.5-50.4 61.9-65.8 99.3-16 38.8-24 79.9-23.6 122.2 0.7 91.7 40.1 177.2 108.1 234.8 3.1 2.6 6 5.1 8.9 7.8 14.9 13.4 58 52.8 112.6 102.7 93.5 85.5 209.9 191.9 257.5 234.2 7 6.1 15.8 9.5 24.9 9.5 9.2 0 18.1-3.4 24.9-9.5 34.5-30.7 105.8-95.9 181.4-165 74.2-67.8 150.9-138 195.8-178.2 69.5-57.9 109.6-144.4 109.9-237.3 0.1-42.5-8-83.6-24-122.2z"
   fill="#8a8a8a"></path>
</svg>

      </i>
    </span><span class="author">
        BGBiao
        
      </span></span>

  
  

  
</div>

    </footer>

    <div class="back-to-top" id="back-to-top">
      <i class="iconfont">
        
        <svg class="icon" viewBox="0 0 1024 1024" version="1.1"
  xmlns="http://www.w3.org/2000/svg" xmlns:xlink="http://www.w3.org/1999/xlink"
  width="35" height="35">
  <path d="M510.866688 227.694839 95.449397 629.218702l235.761562 0-2.057869 328.796468 362.40389 0L691.55698 628.188232l241.942331-3.089361L510.866688 227.694839zM63.840492 63.962777l894.052392 0 0 131.813095L63.840492 195.775872 63.840492 63.962777 63.840492 63.962777zM63.840492 63.962777"></path>
</svg>

      </i>
    </div>
  </div>
  
<script type="text/javascript" src="/lib/jquery/jquery-3.2.1.min.js"></script>
  <script type="text/javascript" src="/lib/slideout/slideout-1.0.1.min.js"></script>




<script type="text/javascript" src="/js/main.638251f4230630f0335d8c6748e53a96f94b72670920b60c09a56fdc8bece214.js" integrity="sha256-Y4JR9CMGMPAzXYxnSOU6lvlLcmcJILYMCaVv3Ivs4hQ=" crossorigin="anonymous"></script>












  
    <script type="text/javascript" src="/js/load-photoswipe.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe.min.js"></script>
    <script type="text/javascript" src="/lib/photoswipe/photoswipe-ui-default.min.js"></script>
  











  <script>
    $("#openSearch, #openSearchMobile").click(function(){
      $(".modal-dialog").addClass("visible");
    });

    $("#closeSearch").click(function(){
      $(".modal-dialog").removeClass("visible");
    });

    $(document).click(function(event) {
    
      if (!$(event.target).closest(".modal-content, #openSearch, #openSearchMobile").length) {
        $("body").find(".modal-dialog").removeClass("visible");
      }
    });
  </script>





</body>
</html>
